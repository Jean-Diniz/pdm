"""
Aplica√ß√£o principal para an√°lise de dados do RU-UFLA
"""

import click
import sys
import os
from pathlib import Path
from typing import Optional

# Adicionar src ao path para imports
sys.path.append(str(Path(__file__).parent))

from config import SparkConfig, DataPaths
from logging_config import setup_logging, get_module_logger
from data_analysis import RUAnalyzer

@click.group()
@click.option(
    "--log-level",
    type=click.Choice(["DEBUG", "INFO", "WARNING", "ERROR"]),
    default="INFO",
    help="N√≠vel de logging",
)
@click.option(
    "--log-to-file/--no-log-to-file",
    default=True,
    help="Se deve salvar logs em arquivo",
)
@click.option(
    "--log-colors/--no-log-colors", default=True, help="Se deve usar cores nos logs"
)
@click.pass_context
def cli(ctx, log_level, log_to_file, log_colors):
    """RU-UFLA Analytics: An√°lise de dados do Restaurante Universit√°rio da UFLA usando PySpark"""

    # Garantir que temos os diret√≥rios necess√°rios
    DataPaths.ensure_directories()

    # Configurar logging
    setup_logging(
        log_level=log_level, log_to_file=log_to_file, enable_colors=log_colors
    )

    # Contexto compartilhado
    ctx.ensure_object(dict)
    ctx.obj["log_level"] = log_level


@cli.command()
@click.option(
    "--master-url",
    help="URL do Spark master (ex: spark://spark-master:7077). Se n√£o especificado, usa configura√ß√£o padr√£o",
)
@click.option("--app-name", default="RU-UFLA-Analytics", help="Nome da aplica√ß√£o Spark")
@click.option(
    "--mode",
    type=click.Choice(["sample", "complete"]),
    default="complete",
    help="Modo de an√°lise: 'sample' para dados de amostra ou 'complete' para dataset completo",
)
@click.option(
    "--periods",
    help="Lista de per√≠odos letivos separados por v√≠rgula (formato: YYYY/S). Ex: 2024/1,2024/2",
)
@click.pass_context
def analyze(ctx, master_url: Optional[str], app_name: str, mode: str, periods: Optional[str]):
    """Executa an√°lise dos dados do RU-UFLA"""

    logger = get_module_logger("main")
    
    # Processar lista de per√≠odos
    periods_list = None
    if periods:
        import re
        
        # Separar per√≠odos por v√≠rgula e limpar espa√ßos
        periods_list = [p.strip() for p in periods.split(',') if p.strip()]
        
        # Padr√£o para validar formato YYYY/S
        period_pattern = r'^\d{4}/[12]$'
        
        # Validar cada per√≠odo
        for period in periods_list:
            if not re.match(period_pattern, period):
                raise click.BadParameter(f"Per√≠odo inv√°lido: {period}. Use formato YYYY/S (ex: 2024/1)")
        
        logger.info(f"Per√≠odos definidos: {', '.join(periods_list)}")
    else:
        logger.info("Nenhum per√≠odo espec√≠fico. Processando todos os dados.")
    
    if mode == "sample":
        logger.info("Iniciando an√°lise com dados de AMOSTRA do RU-UFLA")
        data_file = DataPaths.RU_DATA_SAMPLE
    else:
        logger.info("Iniciando an√°lise COMPLETA dos dados do RU-UFLA")
        data_file = DataPaths.RU_DATA_COMPLETE

    try:
        # Configurar Spark baseado no ambiente
        if master_url:
            os.environ["SPARK_MASTER_URL"] = master_url
            logger.info(f"Usando Spark master: {master_url}")

        # Criar sess√£o Spark
        spark = SparkConfig.get_spark_session(app_name)

        # Verificar se precisa fazer download para an√°lise completa
        if mode == "complete" and not os.path.exists(data_file):
            logger.info("Dataset completo n√£o encontrado. Fazendo download autom√°tico...")
            analyzer = RUAnalyzer(spark)
            analyzer.download_complete_dataset()
            
            # Verificar se o download foi bem-sucedido
            if not os.path.exists(data_file):
                raise FileNotFoundError(f"Falha no download. Dataset n√£o encontrado em: {data_file}")

        # Executar an√°lise
        analyzer = RUAnalyzer(spark)
        analyzer.run_complete_analysis(data_file, periods=periods_list)

        logger.success(f"An√°lise ({mode}) conclu√≠da com sucesso!")

    except Exception as e:
        logger.error(f"Erro durante a an√°lise: {e}")
        raise click.ClickException(f"Falha na an√°lise: {e}")
    finally:
        if "spark" in locals():
            spark.stop()

@cli.command()
@click.option("--master-url", help="URL do Spark master para teste de conectividade")
@click.option(
    "--app-name",
    default="RU-UFLA-Test-Connection",
    help="Nome da aplica√ß√£o Spark para teste",
)
@click.pass_context
def test_spark(ctx, master_url: Optional[str], app_name: str):
    """Testa a conectividade com o cluster Spark"""

    logger = get_module_logger("test")
    logger.info("Testando conectividade com Spark")

    try:
        if master_url:
            os.environ["SPARK_MASTER_URL"] = master_url
            logger.info(f"Testando conex√£o com: {master_url}")

        spark = SparkConfig.get_spark_session(app_name)

        # Teste simples
        test_data = spark.range(1000, numPartitions=4)
        count = test_data.count()

        logger.success(f"‚úÖ Conex√£o com Spark OK - Processou {count:,} registros")
        logger.info(f"Vers√£o do Spark: {spark.version}")
        logger.info(f"Master URL: {spark.conf.get('spark.master')}")
        logger.info(f"Paralelismo: {spark.sparkContext.defaultParallelism} cores")

    except Exception as e:
        logger.error(f"‚ùå Erro de conectividade: {e}")
        raise click.ClickException(f"Falha na conex√£o: {e}")
    finally:
        if "spark" in locals():
            spark.stop()

@cli.command()
@click.option(
    "--format",
    type=click.Choice(["table", "json"]),
    default="table",
    help="Formato de sa√≠da das informa√ß√µes",
)
def info(format):
    """Mostra informa√ß√µes sobre o ambiente e configura√ß√µes"""

    import platform
    import pyspark
    from datetime import datetime

    info_data = {
        "timestamp": datetime.now().isoformat(),
        "python_version": platform.python_version(),
        "pyspark_version": pyspark.__version__,
        "platform": platform.platform(),
        "spark_home": os.environ.get("SPARK_HOME", "N√£o definido"),
        "java_home": os.environ.get("JAVA_HOME", "N√£o definido"),
        "data_paths": {
            "base_dir": DataPaths.BASE_DIR,
            "data_sample": DataPaths.RU_DATA_SAMPLE,
            "results_dir": DataPaths.RESULTS_DIR,
            "metrics_dir": DataPaths.METRICS_DIR,
            "logs_dir": DataPaths.LOGS_DIR,
        },
    }

    if format == "json":
        import json

        click.echo(json.dumps(info_data, indent=2, ensure_ascii=False))
    else:
        click.echo("üîß RU-UFLA Analytics - Informa√ß√µes do Sistema")
        click.echo("=" * 50)
        click.echo(f"Timestamp: {info_data['timestamp']}")
        click.echo(f"Python: {info_data['python_version']}")
        click.echo(f"PySpark: {info_data['pyspark_version']}")
        click.echo(f"Plataforma: {info_data['platform']}")
        click.echo(f"SPARK_HOME: {info_data['spark_home']}")
        click.echo(f"JAVA_HOME: {info_data['java_home']}")
        click.echo()
        click.echo("üìÅ Caminhos de Dados:")
        for key, value in info_data["data_paths"].items():
            click.echo(f"  {key}: {value}")


if __name__ == "__main__":
    cli()
